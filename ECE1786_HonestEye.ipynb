{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#LABELS\n",
        "#0 = FALSE\n",
        "#1 = TRUE"
      ],
      "metadata": {
        "id": "X1zX74gEm0de"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The following code will read a csv that contains a list of wikipedia articles about Canadian cities."
      ],
      "metadata": {
        "id": "pY3X75W2rrPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "id": "ZzpSydTuzokd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import math"
      ],
      "metadata": {
        "id": "6OGc-GHMzq5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/MyDrive/ECE1786/'"
      ],
      "metadata": {
        "id": "fg0RU_06sOQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cities = []\n",
        "with open(path+'cities.csv', newline='') as csvfile:\n",
        "  csvreader = csv.reader(csvfile, delimiter=',')\n",
        "  for row in csvreader:\n",
        "    cities.append(', '.join(row))\n",
        "\n",
        "#print(cities)"
      ],
      "metadata": {
        "id": "cR2F3DQcrnHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The following code will get summary data from a list of wikipedia articles. Then it will split them into sentences and label them with \"1\" for True and write it into a csv file."
      ],
      "metadata": {
        "id": "hkJEhbNerGhk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pasQWAAddEvc"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def get_wikipedia_summary(article_title):\n",
        "    \"\"\"\n",
        "    Fetches the summary of a Wikipedia article using the Wikipedia API.\n",
        "\n",
        "    Parameters:\n",
        "    article_title (str): The title of the Wikipedia article to fetch.\n",
        "\n",
        "    Returns:\n",
        "    str: The summary of the Wikipedia article.\n",
        "    \"\"\"\n",
        "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    PARAMS = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": article_title,\n",
        "        \"prop\": \"extracts\",\n",
        "        \"exintro\": True,\n",
        "        \"explaintext\": True,\n",
        "    }\n",
        "\n",
        "    response = requests.get(URL, params=PARAMS)\n",
        "    data = response.json()\n",
        "\n",
        "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
        "    return page[\"extract\"] if \"extract\" in page else \"Article not found.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_summaries = []\n",
        "article_not_found = []\n",
        "\n",
        "for i in range(len(cities)):\n",
        "  cities[i] = cities[i].replace(\"\\\"\", \"\")\n",
        "  list_of_summaries.append(get_wikipedia_summary(cities[i]))\n",
        "  #print(list_of_summaries[i])\n",
        "  #print()\n",
        "  if(list_of_summaries[i] == \"Article not found.\"):\n",
        "    article_not_found.append(cities[i])\n",
        "\n",
        "for i in article_not_found:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "spNTeP4_t4RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in list_of_summaries:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "1vwmYkp285b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "GObOumJRp--q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sentences = []\n",
        "for article in list_of_summaries:\n",
        "  sentences.extend(sent_tokenize(article))\n",
        "\n",
        "for sample in sentences:\n",
        "  ### Arbitrarily remove samples that are shorter than 50 characters\n",
        "  if (len(sample) < 50):\n",
        "    print(\"removing: \", sample)\n",
        "    sentences.remove(sample)\n",
        "  ### Remove samples that end with :\n",
        "  elif (sample[-1] == \":\"):\n",
        "    print(\"removing: \", sample)\n",
        "    sentences.remove(sample)\n"
      ],
      "metadata": {
        "id": "OUlF00ghpbvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [1]*len(sentences)\n",
        "\n",
        "def zipLabels(sentences, labels):\n",
        "  list_zip = zip(sentences, labels)\n",
        "  zipped_list = list(list_zip)\n",
        "\n",
        "  #for i in zipped_list:\n",
        "  #  print(i)\n",
        "  return zipped_list"
      ],
      "metadata": {
        "id": "KlJIvI9i9pGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zipped_list = zipLabels(sentences, labels)"
      ],
      "metadata": {
        "id": "mHe6coGAOVmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fields = ['sample', 'label']\n",
        "with open(path+'dataset.csv', 'w') as csvfile:\n",
        "  csvwriter = csv.writer(csvfile, delimiter=',')\n",
        "  csvwriter.writerow(fields)\n",
        "  csvwriter.writerows(zipped_list)"
      ],
      "metadata": {
        "id": "PVe5dbLnlps7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The following code will take a set of sentences and negate them using the chat GPT 4 API. The resulting data will be used as samples and labeled as \"0\" for False in the csv file."
      ],
      "metadata": {
        "id": "eKiqX7_6rck_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = []\n",
        "with open(path+'dataset.csv', newline='') as csvfile:\n",
        "  csvreader = csv.reader(csvfile, delimiter=',')\n",
        "  for row in csvreader:\n",
        "    dataset.append(', '.join(row))\n",
        "\n",
        "import math\n",
        "half = math.floor(len(dataset)/2)\n",
        "negdataset = dataset[:half]  # ~50% of the dataset, to be negated\n",
        "dataset = dataset[half:]"
      ],
      "metadata": {
        "id": "oSCOjDi2wzv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove the label from the samples"
      ],
      "metadata": {
        "id": "Xld4ynK8x59R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gptRequest(input):\n",
        "  api_key = \"\"\n",
        "  prompt = \"Reverse the meaning of the input sentence. For example, Input is \\\"Toronto is the most populous city in Canada and the capital city of the Canadian province of Ontario\\\" and the desired output would be: \\\"Toronto is the least populous city in Canada\\\". Try to be creative and change different part of the sentence to reach the goal.\"\n",
        "  client = OpenAI(api_key=api_key)\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": prompt},\n",
        "      {\"role\": \"user\", \"content\": input},\n",
        "    ]\n",
        "  )\n",
        "  return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "XLErFogfzj7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''negatedSentences = []\n",
        "for sentence in negdataset:\n",
        "  negatedSentences.append(gptRequest(sentence[:-3])+\", \"+str(0))''' #Loop to negate half of data, only performed once. Costs ~2.5$ per use"
      ],
      "metadata": {
        "id": "ge8PBetaxu1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' import pandas as pd\n",
        "negdf = pd.DataFrame(negatedSentences)\n",
        "negdf.to_csv(path+'negdataset.csv', header=False)''' # Ran once, to save first csv file"
      ],
      "metadata": {
        "id": "X1bpNJuUc6_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "ndf = pd.read_csv(path+'negdataset.csv')\n",
        "\n",
        "for i in range(len(ndf)):\n",
        "  if i==0:\n",
        "    odf = pd.DataFrame({'sample': [ndf[\"Sample, no., 0\"][i][:-3]] , 'label': [int(ndf[\"Sample, no., 0\"][i][-1])]})\n",
        "  else:\n",
        "    t = pd.DataFrame({'sample': [ndf[\"Sample, no., 0\"][i][:-3]] , 'label': [int(ndf[\"Sample, no., 0\"][i][-1])]})\n",
        "    odf = pd.concat([odf, t], ignore_index=True)\n",
        "\n",
        "\n",
        "completeDataset = pd.read_csv(path+'dataset.csv')\n",
        "newDataset = pd.concat([odf, completeDataset[math.floor(len(completeDataset)/2):]], ignore_index=True)\n",
        "newDataset.to_csv(path+'newDataset.csv', index=False)"
      ],
      "metadata": {
        "id": "eaU4xB7MN9-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The following code will read the newdataset.csv file and group them into samples of 3. Each sample will contain 2 truths and 1 lie for training"
      ],
      "metadata": {
        "id": "zAc487SeOeuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "newDataset = []\n",
        "with open(path+'newDataset.csv', newline='') as csvfile:\n",
        "  csvreader = csv.reader(csvfile, delimiter=',')\n",
        "  for row in csvreader:\n",
        "    newDataset.append(', '.join(row))\n",
        "\n",
        "list_of_truths = []\n",
        "list_of_lies = []\n",
        "\n",
        "for i in newDataset:\n",
        "  if(i[-1] == \"0\"):\n",
        "    list_of_lies.append(i)\n",
        "  else:\n",
        "    list_of_truths.append(i)"
      ],
      "metadata": {
        "id": "JcO3A_BtOffi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def group(lies, truths):\n",
        "  newSamples = []\n",
        "  for i in range(len(lies)):\n",
        "    two_true_sentences = random.sample(truths, 2)\n",
        "    one_lie_sentence = random.sample(lies, 1)\n",
        "    temp = two_true_sentences + one_lie_sentence\n",
        "    newSamples.append(temp)\n",
        "    #print(\"2\", two_true_sentences)\n",
        "    #print(\"1\", one_lie_sentence)\n",
        "\n",
        "  return newSamples\n"
      ],
      "metadata": {
        "id": "5uSgmQQXOhPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_samples(samples):\n",
        "  labels = []\n",
        "  newSentences = []\n",
        "  for sample in samples:\n",
        "    newSentence = \"\"\n",
        "    for i, sent in enumerate(sample):\n",
        "      label = -1\n",
        "      if (sent[-1] == \"0\"):\n",
        "        labels.append(i)\n",
        "      sample[i] = sent[:-3]\n",
        "      newSentence = newSentence + sample[i]\n",
        "    newSentences.append(newSentence)\n",
        "  return newSentences, labels"
      ],
      "metadata": {
        "id": "ef3gITslOjHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tempSamples = group(list_of_lies, list_of_truths)\n",
        "\n",
        "for sample in tempSamples:\n",
        "  #print(\"or\", sample)\n",
        "  random.shuffle(sample)\n",
        "  #print(\"new\", sample)"
      ],
      "metadata": {
        "id": "nyeOnVGPOkTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newSamples, labels = label_samples(tempSamples)\n",
        "print(labels)\n",
        "output = zipLabels(newSamples, labels)"
      ],
      "metadata": {
        "id": "y2IDz_P4OlXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def writeToCSV(data):\n",
        "  fields = ['sample', 'label']\n",
        "  with open(path+'groupDataset.csv', 'w') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile, delimiter=',')\n",
        "    csvwriter.writerow(fields)\n",
        "    csvwriter.writerows(data)"
      ],
      "metadata": {
        "id": "EGhUHnI3OmOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writeToCSV(output)"
      ],
      "metadata": {
        "id": "b9ZRxQaEOnNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation (create bigger trainset using GPT4 to rephrase some sentences)"
      ],
      "metadata": {
        "id": "5RSd7iyQftQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rephrase(input):\n",
        "  api_key = \"\"\n",
        "  prompt = \"Rephrase the input sentence but keep the meaning of the sentence. Try to be creative!\"\n",
        "  client = OpenAI(api_key=api_key)\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": prompt},\n",
        "      {\"role\": \"user\", \"content\": input},\n",
        "    ]\n",
        "  )\n",
        "  return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "LJjEwJ_5f076"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''portion = newDataset.sample(frac = 1)\n",
        "portion = portion[:math.floor(len(newDataset)/2)]\n",
        "portion = portion.reset_index(drop=True)\n",
        "\n",
        "for i, sentence in enumerate(portion[\"sample\"]):\n",
        "  if i==0:\n",
        "    rephrased = pd.DataFrame({'sample': [rephrase(sentence)] , 'label': [int(portion[\"label\"][i])]})\n",
        "  else:\n",
        "    t = pd.DataFrame({'sample': [rephrase(sentence)] , 'label': [int(portion[\"label\"][i])]})\n",
        "    rephrased = pd.concat([rephrased, t], ignore_index=True)\n",
        "\n",
        "rephrased.to_csv(path+'rephrased.csv', index=False)\n",
        "rephrased''' # Used to create more data"
      ],
      "metadata": {
        "id": "TMwcGRvxgP8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Append the new senteces to previous file\n",
        "new = pd.concat([newDataset, rephrased], ignore_index=True)\n",
        "new.to_csv(path+'originalPlusRephrased.csv', index=False)\n",
        "new'''"
      ],
      "metadata": {
        "id": "qt7OrgUo5ib0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting originalPlusRephrased.csv into a 3 class dataset"
      ],
      "metadata": {
        "id": "7uXKSTXEjEKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "oPR = [] # OriginalPlusRephrased\n",
        "with open(path+'originalPlusRephrased.csv', newline='') as csvfile:\n",
        "  csvreader = csv.reader(csvfile, delimiter=',')\n",
        "  for row in csvreader:\n",
        "    oPR.append(', '.join(row))\n",
        "\n",
        "list_of_truths = []\n",
        "list_of_lies = []\n",
        "\n",
        "for i in oPR:\n",
        "  if(i[-1] == \"0\"):\n",
        "    list_of_lies.append(i)\n",
        "  else:\n",
        "    list_of_truths.append(i)"
      ],
      "metadata": {
        "id": "ZjaodkY8jNy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def group(lies, truths):\n",
        "  newSamples = []\n",
        "  for i in range(len(lies)):\n",
        "    two_true_sentences = random.sample(truths, 2)\n",
        "    one_lie_sentence = random.sample(lies, 1)\n",
        "    temp = two_true_sentences + one_lie_sentence\n",
        "    newSamples.append(temp)\n",
        "\n",
        "  return newSamples"
      ],
      "metadata": {
        "id": "TZIVgx9tjNy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_samples(samples):\n",
        "  labels = []\n",
        "  newSentences = []\n",
        "  for sample in samples:\n",
        "    newSentence = \"\"\n",
        "    for i, sent in enumerate(sample):\n",
        "      label = -1\n",
        "      if (sent[-1] == \"0\"):\n",
        "        labels.append(i)\n",
        "      sample[i] = sent[:-3]\n",
        "      newSentence = newSentence + sample[i]\n",
        "    newSentences.append(newSentence)\n",
        "  return newSentences, labels"
      ],
      "metadata": {
        "id": "OlgLbwQqjNy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tempSamples = group(list_of_lies, list_of_truths)\n",
        "\n",
        "for sample in tempSamples:\n",
        "  random.shuffle(sample)"
      ],
      "metadata": {
        "id": "eixZ7iYgjNy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newSamples, labels = label_samples(tempSamples)\n",
        "print(labels)\n",
        "output = zipLabels(newSamples, labels)"
      ],
      "metadata": {
        "id": "GvYpQeqUjNy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def writeToCSV(data):\n",
        "  fields = ['sample', 'label']\n",
        "  with open(path+'groupDatasetOPR.csv', 'w') as csvfile: #groupDataset Original Plus Rephrased\n",
        "    csvwriter = csv.writer(csvfile, delimiter=',')\n",
        "    csvwriter.writerow(fields)\n",
        "    csvwriter.writerows(data)"
      ],
      "metadata": {
        "id": "jewNnGVijNy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writeToCSV(output)"
      ],
      "metadata": {
        "id": "k9A0ItkAjNy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "x-YlkLPySdxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "! pip install -U accelerate\n",
        "! pip install -U transformers\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "eHdIWchwSfVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import TrainerCallback, TrainerControl, TrainerState\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import log_loss\n",
        "# from sklearn.metrics import multiclass_log_loss"
      ],
      "metadata": {
        "id": "Mzdqd3kaSkQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "class ComputeTrainMetricsCallback(TrainerCallback):\n",
        "    def __init__(self, trainer=None):\n",
        "        super().__init__()\n",
        "        self.trainer = trainer\n",
        "        self.train_accuracy = []\n",
        "        self.eval_accuracy = []\n",
        "        self.train_loss = []\n",
        "        self.eval_loss = []\n",
        "\n",
        "    def on_epoch_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "        if self.trainer is None:\n",
        "            raise ValueError(\"Trainer not set for ComputeTrainMetricsCallback\")\n",
        "\n",
        "        # Making predictions on the training dataset\n",
        "        train_preds = self.trainer.predict(self.trainer.train_dataset)\n",
        "        eval_preds = self.trainer.predict(self.trainer.eval_dataset)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        train_labels = train_preds.label_ids\n",
        "        train_preds = np.argmax(train_preds.predictions, axis=1)\n",
        "        train_accuracy = accuracy_score(train_labels, train_preds)\n",
        "        train_loss = log_loss(train_labels, train_preds)\n",
        "\n",
        "        # train_loss = multiclass_log_loss(train_labels, train_preds)\n",
        "\n",
        "        eval_labels = eval_preds.label_ids\n",
        "        eval_preds = np.argmax(eval_preds.predictions, axis=1)\n",
        "        eval_accuracy = accuracy_score(eval_labels, eval_preds)\n",
        "        eval_loss = log_loss(eval_labels, eval_preds)\n",
        "\n",
        "        # train_loss = multiclass_log_loss(eval_labels, eval_preds)\n",
        "\n",
        "        self.train_accuracy.append(train_accuracy)\n",
        "        self.eval_accuracy.append(eval_accuracy)\n",
        "\n",
        "        self.train_loss.append(train_loss)\n",
        "        self.eval_loss.append(eval_loss)\n",
        "\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "o53FhCt12qhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train on 2 class original dataset (original sentences + half of them negated)"
      ],
      "metadata": {
        "id": "KgI6_166nYtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset('csv', data_files=path+'newDataset.csv')\n",
        "ds = ds.shuffle()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"sample\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = ds.map(tokenize_function, batched=True)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].select(range(math.floor(0.7*len((ds[\"train\"][\"sample\"])))))\n",
        "small_eval_dataset = tokenized_datasets[\"train\"].select(range(math.floor(0.7*len((ds[\"train\"][\"sample\"]))), len((ds[\"train\"][\"sample\"]))))\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"/content/test_trainer\", num_train_epochs=6, evaluation_strategy=\"epoch\")\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "train_metrics_callback = ComputeTrainMetricsCallback(trainer=trainer)\n",
        "trainer.add_callback(train_metrics_callback)\n",
        "\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Csb7aAudSmDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(train_metrics_callback.eval_accuracy) + 1)\n",
        "\n",
        "plt.plot(epochs, train_metrics_callback.train_accuracy, 'bo-', label='Training accuracy')\n",
        "plt.plot(epochs, train_metrics_callback.eval_accuracy, 'ro-', label='Validation accuracy')\n",
        "plt.title('Training and Validation Accuracy for 2-class original dataset')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zbThzY0q4O94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(train_metrics_callback.eval_accuracy) + 1)\n",
        "\n",
        "plt.plot(epochs, train_metrics_callback.train_loss, 'bo-', label='Training loss')\n",
        "plt.plot(epochs, train_metrics_callback.eval_loss, 'ro-', label='Validation loss')\n",
        "plt.title('Training and Validation Loss for 2-class original dataset')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CUejWg2_4TkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.predict(small_eval_dataset)\n",
        "# print(small_eval_dataset[\"sample\"][4])\n",
        "# print(small_eval_dataset[\"label\"][4])"
      ],
      "metadata": {
        "id": "1pisYQOxbE0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train on new dataset (original + rephrased sentences)"
      ],
      "metadata": {
        "id": "56lraJ086MSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset('csv', data_files=path+'originalPlusRephrased.csv')\n",
        "ds = ds.shuffle()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"sample\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = ds.map(tokenize_function, batched=True)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].select(range(math.floor(0.7*len((ds[\"train\"][\"sample\"])))))\n",
        "small_eval_dataset = tokenized_datasets[\"train\"].select(range(math.floor(0.7*len((ds[\"train\"][\"sample\"]))), len((ds[\"train\"][\"sample\"]))))\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "# training_args = TrainingArguments(output_dir=\"/content/test_trainer\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/test_trainer\",\n",
        "    num_train_epochs=6,\n",
        "    evaluation_strategy=\"epoch\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "train_metrics_callback = ComputeTrainMetricsCallback(trainer=trainer)\n",
        "trainer.add_callback(train_metrics_callback)\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "V8ieqnDP6LbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(train_metrics_callback.eval_accuracy) + 1)\n",
        "\n",
        "plt.plot(epochs, train_metrics_callback.train_accuracy, 'bo-', label='Training accuracy')\n",
        "plt.plot(epochs, train_metrics_callback.eval_accuracy, 'ro-', label='Validation accuracy')\n",
        "plt.title('Training and Validation Accuracy for 2-class augmented dataset')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xz3gilK6GaKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(train_metrics_callback.eval_accuracy) + 1)\n",
        "\n",
        "plt.plot(epochs, train_metrics_callback.train_loss, 'bo-', label='Training loss')\n",
        "plt.plot(epochs, train_metrics_callback.eval_loss, 'ro-', label='Validation loss')\n",
        "plt.title('Training and Validation Loss for 2-class augmented dataset')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5vel8ZOCy9CT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train for 3 class dataset"
      ],
      "metadata": {
        "id": "K6PN1WXBXjjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "STpqZkLE_7Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "class ComputeTrainMetricsCallback(TrainerCallback):\n",
        "    def __init__(self, trainer=None):\n",
        "        super().__init__()\n",
        "        self.trainer = trainer\n",
        "        self.train_accuracy = []\n",
        "        self.eval_accuracy = []\n",
        "        self.train_loss = []\n",
        "        self.eval_loss = []\n",
        "\n",
        "    def on_epoch_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "        if self.trainer is None:\n",
        "            raise ValueError(\"Trainer not set for ComputeTrainMetricsCallback\")\n",
        "\n",
        "        # Making predictions on the training dataset\n",
        "        train_preds = self.trainer.predict(self.trainer.train_dataset)\n",
        "        eval_preds = self.trainer.predict(self.trainer.eval_dataset)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        train_labels = train_preds.label_ids\n",
        "        train_loss = log_loss(train_labels, train_preds.predictions, labels=train_labels)\n",
        "        train_preds = np.argmax(train_preds.predictions, axis=1)\n",
        "        train_accuracy = accuracy_score(train_labels, train_preds)\n",
        "\n",
        "        eval_labels = eval_preds.label_ids\n",
        "        eval_loss = log_loss(eval_labels, eval_preds.predictions, labels=eval_labels)\n",
        "        eval_preds = np.argmax(eval_preds.predictions, axis=1)\n",
        "        eval_accuracy = accuracy_score(eval_labels, eval_preds)\n",
        "\n",
        "        self.train_accuracy.append(train_accuracy)\n",
        "        self.eval_accuracy.append(eval_accuracy)\n",
        "\n",
        "        self.train_loss.append(train_loss)\n",
        "        self.eval_loss.append(eval_loss)\n",
        "\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "8ikHAIWg_Zao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset('csv', data_files=path+'groupDataset.csv')\n",
        "ds = ds.shuffle()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=3)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "# model.config.num_labels = 3\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"sample\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = ds.map(tokenize_function, batched=True)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].select(range(math.floor(0.7*len((ds[\"train\"][\"sample\"])))))\n",
        "small_eval_dataset = tokenized_datasets[\"train\"].select(range(math.floor(0.7*len((ds[\"train\"][\"sample\"]))), len((ds[\"train\"][\"sample\"]))))\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"/content/test_trainer\", num_train_epochs=6, evaluation_strategy=\"epoch\")\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "train_metrics_callback = ComputeTrainMetricsCallback(trainer=trainer)\n",
        "trainer.add_callback(train_metrics_callback)\n",
        "\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ek9U67vSXhFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(train_metrics_callback.eval_accuracy) + 1)\n",
        "\n",
        "plt.plot(epochs, train_metrics_callback.train_accuracy, 'bo-', label='Training accuracy')\n",
        "plt.plot(epochs, train_metrics_callback.eval_accuracy, 'ro-', label='Validation accuracy')\n",
        "plt.title('Training and Validation Accuracy for 3-class original dataset')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BgMCiybZ7W8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(train_metrics_callback.eval_accuracy) + 1)\n",
        "\n",
        "plt.plot(epochs, train_metrics_callback.train_loss, 'bo-', label='Training loss')\n",
        "plt.plot(epochs, train_metrics_callback.eval_loss, 'ro-', label='Validation loss')\n",
        "plt.title('Training and Validation Loss for 3-class original dataset')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3QCqnCae7Z9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train on new 3 class dataset (original + rephrased sentences)"
      ],
      "metadata": {
        "id": "JXGnK1qFNgin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset('csv', data_files=path+'groupDatasetOPR.csv')\n",
        "ds = ds.shuffle()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=3)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "# model.config.num_labels = 3\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"sample\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = ds.map(tokenize_function, batched=True)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].select(range(math.floor(0.7*len((ds[\"train\"][\"sample\"])))))\n",
        "small_eval_dataset = tokenized_datasets[\"train\"].select(range(math.floor(0.7*len((ds[\"train\"][\"sample\"]))), len((ds[\"train\"][\"sample\"]))))\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"/content/test_trainer\", num_train_epochs=6, evaluation_strategy=\"epoch\")\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "train_metrics_callback = ComputeTrainMetricsCallback(trainer=trainer)\n",
        "trainer.add_callback(train_metrics_callback)\n",
        "\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "S4ivV23nNiAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(train_metrics_callback.eval_accuracy) + 1)\n",
        "\n",
        "plt.plot(epochs, train_metrics_callback.train_accuracy, 'bo-', label='Training accuracy')\n",
        "plt.plot(epochs, train_metrics_callback.eval_accuracy, 'ro-', label='Validation accuracy')\n",
        "plt.title('Training and Validation Accuracy for 3-class augmented dataset')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fJHRINhhNl5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(train_metrics_callback.eval_accuracy) + 1)\n",
        "\n",
        "plt.plot(epochs, train_metrics_callback.train_loss, 'bo-', label='Training loss')\n",
        "plt.plot(epochs, train_metrics_callback.eval_loss, 'ro-', label='Validation loss')\n",
        "plt.title('Training and Validation Loss for 3-class augmented dataset')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mSWTzvU8NqcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train for 3 epochs"
      ],
      "metadata": {
        "id": "1H9g6DLmSOZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "rpjHJTgGSQrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "! pip install -U accelerate\n",
        "! pip install -U transformers\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "QqT72A7CSQrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import TrainerCallback, TrainerControl, TrainerState\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import log_loss\n",
        "# from sklearn.metrics import multiclass_log_loss"
      ],
      "metadata": {
        "id": "-NdeRr0ASQrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "class ComputeTrainMetricsCallback(TrainerCallback):\n",
        "    def __init__(self, trainer=None):\n",
        "        super().__init__()\n",
        "        self.trainer = trainer\n",
        "        self.train_accuracy = []\n",
        "        self.eval_accuracy = []\n",
        "        self.train_loss = []\n",
        "        self.eval_loss = []\n",
        "\n",
        "    def on_epoch_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "        if self.trainer is None:\n",
        "            raise ValueError(\"Trainer not set for ComputeTrainMetricsCallback\")\n",
        "\n",
        "        # Making predictions on the training dataset\n",
        "        train_preds = self.trainer.predict(self.trainer.train_dataset)\n",
        "        eval_preds = self.trainer.predict(self.trainer.eval_dataset)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        train_labels = train_preds.label_ids\n",
        "        train_preds = np.argmax(train_preds.predictions, axis=1)\n",
        "        train_accuracy = accuracy_score(train_labels, train_preds)\n",
        "        train_loss = log_loss(train_labels, train_preds)\n",
        "\n",
        "        # train_loss = multiclass_log_loss(train_labels, train_preds)\n",
        "\n",
        "        eval_labels = eval_preds.label_ids\n",
        "        eval_preds = np.argmax(eval_preds.predictions, axis=1)\n",
        "        eval_accuracy = accuracy_score(eval_labels, eval_preds)\n",
        "        eval_loss = log_loss(eval_labels, eval_preds)\n",
        "\n",
        "        # train_loss = multiclass_log_loss(eval_labels, eval_preds)\n",
        "\n",
        "        self.train_accuracy.append(train_accuracy)\n",
        "        self.eval_accuracy.append(eval_accuracy)\n",
        "\n",
        "        self.train_loss.append(train_loss)\n",
        "        self.eval_loss.append(eval_loss)\n",
        "\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "cEXNO9AvSQrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train on 2 class original dataset (original sentences + half of them negated)"
      ],
      "metadata": {
        "id": "kq0L5ZCDSQrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset('csv', data_files=path+'newDataset.csv')\n",
        "ds = ds.shuffle()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"sample\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = ds.map(tokenize_function, batched=True)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].select(range(math.floor(0.7*len((ds[\"train\"][\"sample\"])))))\n",
        "small_eval_dataset = tokenized_datasets[\"train\"].select(range(math.floor(0.7*len((ds[\"train\"][\"sample\"]))), len((ds[\"train\"][\"sample\"]))))\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"/content/test_trainer\", num_train_epochs=3, evaluation_strategy=\"epoch\")\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "train_metrics_callback = ComputeTrainMetricsCallback(trainer=trainer)\n",
        "trainer.add_callback(train_metrics_callback)\n",
        "\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "LaRgeM_MSQrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(train_metrics_callback.eval_accuracy) + 1)\n",
        "\n",
        "plt.plot(epochs, train_metrics_callback.train_accuracy, 'bo-', label='Training accuracy')\n",
        "plt.plot(epochs, train_metrics_callback.eval_accuracy, 'ro-', label='Validation accuracy')\n",
        "plt.title('Training and Validation Accuracy for 2-class original dataset')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3iAuJgxGSQrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(train_metrics_callback.eval_accuracy) + 1)\n",
        "\n",
        "plt.plot(epochs, train_metrics_callback.train_loss, 'bo-', label='Training loss')\n",
        "plt.plot(epochs, train_metrics_callback.eval_loss, 'ro-', label='Validation loss')\n",
        "plt.title('Training and Validation Loss for 2-class original dataset')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tuDXi8srSQrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.predict(small_eval_dataset)\n",
        "# print(small_eval_dataset[\"sample\"][4])\n",
        "# print(small_eval_dataset[\"label\"][4])"
      ],
      "metadata": {
        "id": "gtmbYO7-SQrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train on new dataset (original + rephrased sentences)"
      ],
      "metadata": {
        "id": "891wlPkfSQrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset('csv', data_files=path+'originalPlusRephrased.csv')\n",
        "ds = ds.shuffle()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"sample\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = ds.map(tokenize_function, batched=True)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].select(range(math.floor(0.7*len((ds[\"train\"][\"sample\"])))))\n",
        "small_eval_dataset = tokenized_datasets[\"train\"].select(range(math.floor(0.7*len((ds[\"train\"][\"sample\"]))), len((ds[\"train\"][\"sample\"]))))\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "# training_args = TrainingArguments(output_dir=\"/content/test_trainer\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/test_trainer\",\n",
        "    num_train_epochs=3,\n",
        "    evaluation_strategy=\"epoch\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "train_metrics_callback = ComputeTrainMetricsCallback(trainer=trainer)\n",
        "trainer.add_callback(train_metrics_callback)\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "AKyDmORYSQrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(train_metrics_callback.eval_accuracy) + 1)\n",
        "\n",
        "plt.plot(epochs, train_metrics_callback.train_accuracy, 'bo-', label='Training accuracy')\n",
        "plt.plot(epochs, train_metrics_callback.eval_accuracy, 'ro-', label='Validation accuracy')\n",
        "plt.title('Training and Validation Accuracy for 2-class augmented dataset')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4cC1H4YmSQrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(train_metrics_callback.eval_accuracy) + 1)\n",
        "\n",
        "plt.plot(epochs, train_metrics_callback.train_loss, 'bo-', label='Training loss')\n",
        "plt.plot(epochs, train_metrics_callback.eval_loss, 'ro-', label='Validation loss')\n",
        "plt.title('Training and Validation Loss for 2-class augmented dataset')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "baDTPLmoSQrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train for 3 class dataset"
      ],
      "metadata": {
        "id": "DXB0Wd9oSQrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "MlDg6j8iSQrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "class ComputeTrainMetricsCallback(TrainerCallback):\n",
        "    def __init__(self, trainer=None):\n",
        "        super().__init__()\n",
        "        self.trainer = trainer\n",
        "        self.train_accuracy = []\n",
        "        self.eval_accuracy = []\n",
        "        self.train_loss = []\n",
        "        self.eval_loss = []\n",
        "\n",
        "    def on_epoch_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "        if self.trainer is None:\n",
        "            raise ValueError(\"Trainer not set for ComputeTrainMetricsCallback\")\n",
        "\n",
        "        # Making predictions on the training dataset\n",
        "        train_preds = self.trainer.predict(self.trainer.train_dataset)\n",
        "        eval_preds = self.trainer.predict(self.trainer.eval_dataset)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        train_labels = train_preds.label_ids\n",
        "        train_loss = log_loss(train_labels, train_preds.predictions, labels=train_labels)\n",
        "        train_preds = np.argmax(train_preds.predictions, axis=1)\n",
        "        train_accuracy = accuracy_score(train_labels, train_preds)\n",
        "\n",
        "        eval_labels = eval_preds.label_ids\n",
        "        eval_loss = log_loss(eval_labels, eval_preds.predictions, labels=eval_labels)\n",
        "        eval_preds = np.argmax(eval_preds.predictions, axis=1)\n",
        "        eval_accuracy = accuracy_score(eval_labels, eval_preds)\n",
        "\n",
        "        self.train_accuracy.append(train_accuracy)\n",
        "        self.eval_accuracy.append(eval_accuracy)\n",
        "\n",
        "        self.train_loss.append(train_loss)\n",
        "        self.eval_loss.append(eval_loss)\n",
        "\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "fWNTIoDSSQrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset('csv', data_files=path+'groupDataset.csv')\n",
        "ds = ds.shuffle()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=3)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "# model.config.num_labels = 3\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"sample\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = ds.map(tokenize_function, batched=True)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].select(range(math.floor(0.7*len((ds[\"train\"][\"sample\"])))))\n",
        "small_eval_dataset = tokenized_datasets[\"train\"].select(range(math.floor(0.7*len((ds[\"train\"][\"sample\"]))), len((ds[\"train\"][\"sample\"]))))\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"/content/test_trainer\", num_train_epochs=3, evaluation_strategy=\"epoch\")\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "train_metrics_callback = ComputeTrainMetricsCallback(trainer=trainer)\n",
        "trainer.add_callback(train_metrics_callback)\n",
        "\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "fjh3kzSESQrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(train_metrics_callback.eval_accuracy) + 1)\n",
        "\n",
        "plt.plot(epochs, train_metrics_callback.train_accuracy, 'bo-', label='Training accuracy')\n",
        "plt.plot(epochs, train_metrics_callback.eval_accuracy, 'ro-', label='Validation accuracy')\n",
        "plt.title('Training and Validation Accuracy for 3-class original dataset')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cwq5BgsiSQrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(train_metrics_callback.eval_accuracy) + 1)\n",
        "\n",
        "plt.plot(epochs, train_metrics_callback.train_loss, 'bo-', label='Training loss')\n",
        "plt.plot(epochs, train_metrics_callback.eval_loss, 'ro-', label='Validation loss')\n",
        "plt.title('Training and Validation Loss for 3-class original dataset')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BEOfC4-sSQrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train on new 3 class dataset (original + rephrased sentences)"
      ],
      "metadata": {
        "id": "gYTvQIZwSQrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset('csv', data_files=path+'groupDatasetOPR.csv')\n",
        "ds = ds.shuffle()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=3)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "# model.config.num_labels = 3\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"sample\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = ds.map(tokenize_function, batched=True)\n",
        "\n",
        "small_train_dataset = tokenized_datasets[\"train\"].select(range(math.floor(0.7*len((ds[\"train\"][\"sample\"])))))\n",
        "small_eval_dataset = tokenized_datasets[\"train\"].select(range(math.floor(0.7*len((ds[\"train\"][\"sample\"]))), len((ds[\"train\"][\"sample\"]))))\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"/content/test_trainer\", num_train_epochs=3, evaluation_strategy=\"epoch\")\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "train_metrics_callback = ComputeTrainMetricsCallback(trainer=trainer)\n",
        "trainer.add_callback(train_metrics_callback)\n",
        "\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "GML6o_7LSQrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(train_metrics_callback.eval_accuracy) + 1)\n",
        "\n",
        "plt.plot(epochs, train_metrics_callback.train_accuracy, 'bo-', label='Training accuracy')\n",
        "plt.plot(epochs, train_metrics_callback.eval_accuracy, 'ro-', label='Validation accuracy')\n",
        "plt.title('Training and Validation Accuracy for 3-class augmented dataset')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qsNcztVySQrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(train_metrics_callback.eval_accuracy) + 1)\n",
        "\n",
        "plt.plot(epochs, train_metrics_callback.train_loss, 'bo-', label='Training loss')\n",
        "plt.plot(epochs, train_metrics_callback.eval_loss, 'ro-', label='Validation loss')\n",
        "plt.title('Training and Validation Loss for 3-class augmented dataset')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aJwWaMxgSQrS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}